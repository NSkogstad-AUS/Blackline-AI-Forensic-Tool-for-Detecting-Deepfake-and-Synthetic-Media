# Backend pipeline (Windows PowerShell) — copy/paste ready

# 0) Activate venv (if not already)
# python -m venv .venv
# .\.venv\Scripts\Activate.ps1
# pip install -r documentation/requirements.txt

# 1) Ingest
python -m backend.src.ingest .\backend\datasets --store .\backend\data\raw

# 2) Probe container/streams (ffprobe) + EXIF (optional)
# Add --no-exif for speed if needed
python -m backend.src.probe_media `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\probe.jsonl

# 3) Validate format & safe decode
python -m backend.src.validate_media `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\validate.jsonl

# 4) Scene/shot detection → shots.jsonl
# threshold: 0.30–0.50 typical; min-shot-ms merges very short shots
python -m backend.src.scene_detect `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\shots.jsonl `
  --threshold 0.30 `
  --min-shot-ms 400

# 5) Per-shot frame sampling (frames.jsonl + JPEGs)
# Frames only (Lowered values for quick testing at the moment)
python -m backend.src.sample_frames `
  --shots       .\backend\data\derived\shots.jsonl `
  --frames-out  .\backend\data\derived\frames.jsonl `
  --frames-root .\backend\data\derived\frames `
  --fps 2 `
  --jpeg-quality 85 `
  --limit 400

# (Optional) Also extract clips per shot
python -m backend.src.sample_frames `
  --shots       .\backend\data\derived\shots.jsonl `
  --frames-out  .\backend\data\derived\frames.jsonl `
  --frames-root .\backend\data\derived\frames `
  --fps 8 `
  --jpeg-quality 90 `
  --extract-clips `
  --clips-root .\backend\data\derived\clips

# 6) Normalize frames & compute quality metrics (Step 5 of code guide)
# Standardize to 299x299, compute blur/brightness/pHash, optional face crops
python -m backend.src.normalize_frames `
  --frames       .\backend\data\derived\frames.jsonl `
  --frames-root  .\backend\data\derived\frames `
  --out          .\backend\data\derived\frames_normalized.jsonl `
  --normalized-root .\backend\data\derived\normalized `
  --target-size 299 `
  --extract-faces

# 7) Build manifest CSV (labels + probe/validate summaries)
python -m backend.src.build_manifest `
  --audit    .\backend\data\audit\ingest_log.jsonl `
  --out      .\backend\data\derived\manifest.csv `
  --meta     .\backend\datasets\train\metadata.json `
  --probe    .\backend\data\derived\probe.jsonl `
  --validate .\backend\data\derived\validate.jsonl

# 8) Analyze metadata (sanity checks + suspicion score)
python -m backend.src.analyze_metadata `
  --probe .\backend\data\derived\probe.jsonl `
  --out   .\backend\data\derived\metadata_analysis.jsonl

# 9) ELA (Error Level Analysis) over normalized frames
# Quick test: limit to first 200 frames
python -m backend.src.models.ela_detector.compute_ela `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_ela.jsonl `
  --overlays .\backend\data\derived\overlays `
  --jpeg-quality 85 `
  --scale 10 `
  --limit 200

# 9a) ELA over sampled frames (raw, recommended for strongest signal)
# Quick test: limit to first 200 frames
python -m backend.src.models.ela_detector.compute_ela `
  --frames   .\backend\data\derived\frames.jsonl `
  --root     .\backend\data\derived\frames `
  --out      .\backend\data\derived\frames_ela_raw.jsonl `
  --overlays .\backend\data\derived\overlays `
  --jpeg-quality 85 `
  --scale 10 `
  --limit 200

# Full ELA run (all frames, defaults ok)
# python -m backend.src.models.ela_detector.compute_ela --jpeg-quality 85 --scale 10

#9b) ELA analysis
python -m backend.src.models.ela_detector.eval_ela `
  --ela      .\backend\data\derived\frames_ela_raw.jsonl `
  --manifest .\backend\data\derived\manifest.csv `
  --cv-k 5 `
  --seed 42 `
  --agg median `
  --metric ela_error_mean `
  --out .\backend\data\derived\ela_eval_cv.json

# 10) XceptionNet (timm) — per-frame scoring + optional shot aggregation
# (Optional) Provide deepfake-trained weights:
#   --checkpoint path\to\deepfake_xception.pth
# ...existing code...

# 10) Copy-Move detection over normalized frames (NOT USING ANYMORE!!!!!!)
# Quick test: limit to first 200 frames
python -m backend.src.models.compute_copy_move `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_copy_move.jsonl `
  --overlays .\backend\data\derived\overlays `
  --limit 200

# Full Copy-Move run (all frames, defaults ok)
# python -m backend.src.models.compute_copy_move

# 11) Noise analysis over normalized frames
# Quick test: limit to first 200 frames
python -m backend.src.models.compute_noise `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_noise.jsonl `
  --overlays .\backend\data\derived\overlays `
  --limit 200

# Full Noise analysis run (all frames, defaults ok)
# python .\backend\src\models\compute_noise.py

# 12) Deepfake detection (currently stubbed)
# Quick test: limit to first 50 frames
python -m backend.src.models.df_detector `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_deepfake.jsonl `
  --limit 50

# Full Deepfake detection run (all frames, defaults ok)
# python -m backend.src.models.df_detector

# 13) FastAPI server for web interface
# Run the web API server (requires uvicorn)
# uvicorn backend.src.api_server:app --reload --port 8000
