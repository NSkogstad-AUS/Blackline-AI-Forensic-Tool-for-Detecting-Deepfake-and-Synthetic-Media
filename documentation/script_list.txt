# 0) Activate venv (if not already)
# python -m venv .venv
# .\.venv\Scripts\Activate.ps1
# pip install -r documentation/requirements.txt

# 1) Ingest
python -m backend.src.ingest .\backend\datasets --store .\backend\data\raw

# 2) Probe container/streams (ffprobe) + EXIF (optional)
# Add --no-exif for speed if needed
python -m backend.src.probe_media `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\probe.jsonl

# 3) Validate format & safe decode
python -m backend.src.validate_media `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\validate.jsonl

# 4) Scene/shot detection → shots.jsonl
# threshold: 0.30–0.50 typical; min-shot-ms merges very short shots
python -m backend.src.scene_detect `
  --audit .\backend\data\audit\ingest_log.jsonl `
  --out   .\backend\data\derived\shots.jsonl `
  --threshold 0.30 `
  --min-shot-ms 400

# 5) Per-shot frame sampling (frames.jsonl + JPEGs)
# Frames only (lowered values for quick testing)
python -m backend.src.sample_frames `
  --shots       .\backend\data\derived\shots.jsonl `
  --frames-out  .\backend\data\derived\frames.jsonl `
  --frames-root .\backend\data\derived\frames `
  --fps 2 `
  --jpeg-quality 85 `
  --limit 400

# (Optional) Also extract clips per shot (for clip-based models)
python -m backend.src.sample_frames `
  --shots       .\backend\data\derived\shots.jsonl `
  --frames-out  .\backend\data\derived\frames.jsonl `
  --frames-root .\backend\data\derived\frames `
  --fps 8 `
  --jpeg-quality 90 `
  --extract-clips `
  --clips-root .\backend\data\derived\clips

# 5b) Build clips.jsonl from shots + extracted clips
python -m backend.src.build_clips_jsonl `
  --shots .\backend\data\derived\shots.jsonl `
  --clips-root .\backend\data\derived\clips `
  --out .\backend\data\derived\clips.jsonl

# 6) Normalize frames & compute quality metrics
# Standardize to 299x299, compute blur/brightness/pHash, optional face crops
python -m backend.src.normalize_frames `
  --frames       .\backend\data\derived\frames.jsonl `
  --frames-root  .\backend\data\derived\frames `
  --out          .\backend\data\derived\frames_normalized.jsonl `
  --normalized-root .\backend\data\derived\normalized `
  --target-size 299 `
  --extract-faces

# 7) Build manifest CSV (labels + probe/validate summaries)
python -m backend.src.build_manifest `
  --audit    .\backend\data\audit\ingest_log.jsonl `
  --out      .\backend\data\derived\manifest.csv `
  --meta     .\backend\datasets\train\metadata.json `
  --probe    .\backend\data\derived\probe.jsonl `
  --validate .\backend\data\derived\validate.jsonl

# 8) Analyze metadata (sanity checks + suspicion score)
python -m backend.src.analyze_metadata `
  --probe .\backend\data\derived\probe.jsonl `
  --out   .\backend\data\derived\metadata_analysis.jsonl

# 9) ELA (Error Level Analysis) over normalized frames (quick test)
python -m backend.src.models.ela_detector.compute_ela `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_ela.jsonl `
  --overlays .\backend\data\derived\overlays `
  --jpeg-quality 85 `
  --scale 10 `
  --limit 200

# 9a) ELA over raw sampled frames (often stronger)
python -m backend.src.models.ela_detector.compute_ela `
  --frames   .\backend\data\derived\frames.jsonl `
  --root     .\backend\data\derived\frames `
  --out      .\backend\data\derived\frames_ela_raw.jsonl `
  --overlays .\backend\data\derived\overlays `
  --jpeg-quality 85 `
  --scale 10 `
  --limit 200

# 9b) ELA evaluation (CV on manifest labels)
python -m backend.src.models.ela_detector.eval_ela `
  --ela      .\backend\data\derived\frames_ela_raw.jsonl `
  --manifest .\backend\data\derived\manifest.csv `
  --cv-k 5 `
  --seed 42 `
  --agg median `
  --metric ela_error_mean `
  --out .\backend\data\derived\ela_eval_cv.json

# 10) Copy-Move detection (deprecated in your note, kept for reference)
python -m backend.src.models.compute_copy_move `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_copy_move.jsonl `
  --overlays .\backend\data\derived\overlays `
  --limit 200

# 11) Noise analysis over normalized frames (quick test)
python -m backend.src.models.compute_noise `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_noise.jsonl `
  --overlays .\backend\data\derived\overlays `
  --limit 200

# 12) Deepfake detection (stubbed hash model over frames)
python -m backend.src.models.df_detector `
  --frames   .\backend\data\derived\frames_normalized.jsonl `
  --root     .\backend\data\derived\normalized `
  --out      .\backend\data\derived\frames_deepfake.jsonl `
  --limit 50

# 13) FastAPI server for web interface
# uvicorn backend.src.api_server:app --reload --port 8000

##### DEEP MODELS — INFERENCE & TRAINING #####

# 14) Xception (timm) — per-frame inference (P(FAKE)), optional shot aggregation
python -m backend.src.models.xception_infer `
  --frames-jsonl .\backend\data\derived\frames_normalized.jsonl `
  --root .\backend\data\derived `
  --out .\backend\data\derived\xception_scores_frames.jsonl `
  --checkpoint .\backend\models\xception_v1.pth `
  --batch-size 32 --num-workers 0 `
  --use-faces `
  --aggregate-shot-out .\backend\data\derived\xception_scores_shots.jsonl

# 15) Xception — training (fast fine-tune; saves best threshold in checkpoint meta)
python -m backend.src.models.xception_train `
  --frames-jsonl .\backend\data\derived\frames_normalized.jsonl `
  --root .\backend\data\derived `
  --manifest .\backend\data\derived\manifest.csv `
  --id-col sha256 --label-col label_num `
  --epochs 8 --batch-size 32 --lr-head 1e-3 --lr-backbone 3e-4 `
  --out .\backend\models\xception_v1.pth `
  --num-workers 4

# 16) TimeSformer — per-clip inference (requires clips.jsonl from step 5b)
python -m backend.src.models.timesformer_infer `
  --clips .\backend\data\derived\clips.jsonl `
  --clips-root .\backend\data\derived\clips `
  --checkpoint .\backend\models\timesformer_v1.pt `
  --config .\backend\models\timesformer_v1.config.json `
  --out .\backend\data\derived\timesformer_scores.jsonl `
  --frames 8 --size 224 --batch-size 2 --device auto

# 17) TimeSformer — training (saves .pt + .config.json with threshold)
python -m backend.src.models.timesformer_train `
  --clips .\backend\data\derived\clips.jsonl `
  --clips-root .\backend\data\derived\clips `
  --manifest .\backend\data\derived\manifest.csv `
  --out .\backend\models\timesformer_v1.pt `
  --config .\backend\models\timesformer_v1.config.json `
  --epochs 10 --batch-size 4 --frames 16 --size 224

# 18) Fusion (blend) — grid-search α (Xception p95 + TimeSformer median)
python -m backend.src.models.fuse_models `
  --xception .\backend\data\derived\xception_scores_frames.jsonl `
  --timesformer .\backend\data\derived\timesformer_scores.jsonl `
  --manifest .\backend\data\derived\manifest.csv `
  --agg-frame p95 --agg-clip median `
  --learn-alpha `
  --use-split --val-as-test `
  --out .\backend\data\derived\fusion_predictions.jsonl

# 19) Fusion (stacking) — Logistic Regression (+ optional calibration)
python -m backend.src.models.fuse_models_lr `
  --xception .\backend\data\derived\xception_scores_frames.jsonl `
  --timesformer .\backend\data\derived\timesformer_scores.jsonl `
  --manifest .\backend\data\derived\manifest.csv `
  --agg-frame p95 --agg-clip median `
  --C 1.0 --class-weight balanced --calibrate sigmoid `
  --use-split --val-as-test `
  --out .\backend\data\derived\fusion_predictions_lr.jsonl

# 20) Evaluation — learn threshold on TRAIN, evaluate on TEST; save curves+JSON
python -m backend.src.eval.eval_jsonl `
  --preds .\backend\data\derived\fusion_predictions.jsonl `
  --manifest .\backend\data\derived\manifest.csv `
  --learn-threshold train --split test `
  --plots-out-dir .\backend\data\derived\plots `
  --out-metrics .\backend\data\derived\metrics_fusion.json
