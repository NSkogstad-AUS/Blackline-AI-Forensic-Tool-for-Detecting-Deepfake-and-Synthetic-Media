Why each step is needed

backend testing

1) Intake → immutability → hashing
What you do: Accept the file, compute a SHA-256, store the raw upload under that hash, log who/when/how.
Why it matters: This establishes chain-of-custody and lets you prove the exact bytes you analyzed never changed. Your spec explicitly asks for hashing to verify integrity and flag tampering. 
 
2) Format validation & safe decoding
What you do: Validate container/codec before touching frames; reject weird or broken files.
Why it matters: Malformed media can crash tools or produce misleading frames. Your “Preprocessing” section calls out format validation up front. 

3) Probe & harvest metadata (ffprobe / ExifTool / MediaInfo)
What you do: Run ffprobe for container/streams/timebases and ExifTool for EXIF/device/GPS/timestamps; store raw JSON.
Why it matters: Inconsistencies (missing EXIF, non-monotonic timecodes, odd re-encode chains) are classic manipulation clues and must appear in reports. Your brief requires EXIF and container analysis with FFmpeg + hashing. 

4) Shot detection → frame/clip sampling
What you do: Detect scene changes, keep keyframes, then sample frames (e.g., 6–12 fps) per shot; extract short clips for temporal models.
Why it matters: Uniform sampling across the whole video can miss short manipulations and bias you toward long scenes. Shot-aware sampling makes temporal cues (blink/pose/jitter) measurable—the exact anomalies you list. 

5) Normalization & basic quality signals
What you do: Convert to a standard size/colorspace; compute blur/brightness/pHash; cache face crops if you use face-centric models.
Why it matters: Normalization removes confounders (resolution/codec) so models learn manipulation cues, not camera quirks. Quality metrics let you down-weight awful frames and de-dupe near-identicals. Preprocessing (resizing/normalisation) is explicitly in scope. 

6) Classical forensic features (ELA, noise, LBP, copy-move)
What you do: Precompute ELA maps, noise/FFT/SRM residuals, LBP textures, and copy-move matches.
Why it matters: These signals are orthogonal to deep nets, great for explainability, and often survive heavy compression. Your brief calls out ELA, noise analysis, LBP, and copy-move explicitly. 
 
7) Temporal inconsistency analysis
What you do: Track blinks, head pose, facial landmarks; compute per-clip temporal scores.
Why it matters: Many deepfakes break bio-mechanics and motion coherence (blinks too regular/rare, pose jitter). Your brief lists temporal anomalies as a core technique. 

8) Audio–visual sync checks (optional but valuable)
What you do: Extract phonemes (ASR) and align to lip motion; score lip-sync and coarticulation.
Why it matters: Voice swaps and “retalking” often drift or show non-human coarticulation. It complements video-only cues. (It folds under “detailed forensic analysis of video sequences” in your objective.) 

9) Deep models (spatial + temporal + ensemble)
What you do: Use a strong frame detector (transformer or CNN) and a temporal detector (3D/transformer) and fuse their scores.
Why it matters: Your requirements include CNNs, GAN-detection, autoencoders, 3D CNNs/LRCNs, and Transformers for video. A single model family overfits to a specific generator or dataset; multi-branch ensembles generalize better and fail more gracefully. 

10) Confidence scoring, calibration, and thresholds
What you do: Convert logits to calibrated probabilities and define “Likely Real / Unsure / Likely Manipulated” bands.
Why it matters: For investigators, high-stakes calls need calibrated confidence with a documented thresholding policy—your spec calls for “confidence scoring and anomaly flagging.” 

11) Storage & logging (tables + object store)
What you do: Persist raw uploads, frames/clips, features, overlays, per-module scores, and full audit logs.
Why it matters: Reproducibility and traceability. Your “Storage/Logging” requirement explicitly mentions saving metadata, logs, and results in a DB (PostgreSQL/SQLite). 

12) Visualization & reporting
What you do: Heatmaps, ELA overlays, timelines; export PDF/JSON reports with hashes, tool versions, and all evidence.
Why it matters: Investigators need to see why, not just a score; reports are artifacts you can hand over. Your brief mandates heatmaps/overlays and exportable PDF/JSON. 
 

Why not “just GANs / CNNs / autoencoders / deep learning” (recap)
You are using deep learning—it’s in your requirements (CNNs, 3D CNNs/LRCNs, Transformers). But a single family is brittle: CNN-only systems often lose accuracy on new datasets/generators; GAN-specific detectors can miss diffusion fakes; autoencoder reconstruction error vanishes on near-real fakes.

Classical forensics + metadata are orthogonal and explainable. They back up or contradict the ML score in ways that are hard to spoof all at once—and that courts understand. 
 
 

One-page study roadmap (10–12 focused days)
Day 1 — Integrity & hashing

Concepts: chain-of-custody, SHA-256 vs MD5, immutable “bronze” storage.

Hands-on: compute hashes; design an intake record (who/when/hash/original name).

Success: you can prove a file hasn’t changed.

Day 2 — Containers & probing

Concepts: container vs codec, timebase, PTS/DTS.

Hands-on: run ffprobe and read stream metadata; stash raw JSON.

Day 3 — EXIF & header anomalies

Concepts: EXIF vs container tags; common inconsistencies (device/timestamps/GPS).

Hands-on: run ExifTool; diff EXIF across edited vs original.

Day 4 — Shots & sampling

Concepts: why scene detection beats uniform sampling; keyframe logic.

Hands-on: run a scene-detector; export per-shot frame lists.

Day 5 — Preprocessing & quality

Concepts: resizing/normalisation pitfalls; blur/brightness/pHash uses.

Hands-on: compute these metrics; build a quick Parquet table.

Day 6 — ELA & noise residuals

Concepts: JPEG recompression errors; SRM/FFT residuals; when ELA lies.

Hands-on: generate maps; learn how to visualize them.

Day 7 — LBP, copy-move

Concepts: texture descriptors; block matching for duplicates.

Hands-on: run copy-move on a tampered image; inspect matches.

Day 8 — Temporal cues

Concepts: blinks, head pose, landmark stability.

Hands-on: extract landmarks; compute a simple blink/pose score per clip.

Day 9 — Spatial detector

Concepts: CNN vs transformer; cross-dataset generalization.

Hands-on: finetune a small frame detector; export heatmaps.

Day 10 — Temporal detector + fusion

Concepts: 3D/transformer windows; late fusion; calibration.

Hands-on: train a small temporal head on 16–32-frame clips; temperature scale; define thresholds.

Day 11 — Metadata/provenance in reports

Concepts: how to write a defensible PDF/JSON (hashes, versions, evidence).

Hands-on: generate a report for two known cases (one real, one fake).

Day 12 — QA & data governance

Concepts: leakage, subject-disjoint splits, Great Expectations.

Hands-on: write expectations (duration>0, fps ranges, label balance); run them on your tables.

Data-schema cheat sheet (pin this)
assets — one row per file
asset_id | sha256 | source | license | media_type | duration_ms | width | height | fps | container | upload_time | split | label | probe_json | exif_json | provenance_status
Why: the canonical “what we got” + raw tool outputs for traceability. Matches your “Media Upload / Metadata Inspection” needs. 

shots — scene structure for videos
shot_id | asset_id | t_start_ms | t_end_ms | detector | confidence
Why: unbiased temporal sampling and clip creation for temporal detectors. 

frames — sampled frames (and crops)
frame_id | asset_id | shot_id | t_ms | keyframe | uri | sha1 | phash | blur | luminance | faces_json
Why: standardized inputs; quality controls; fast joins to detections.

detections — per-module results
asset_id | window_id | module_name | module_version | t_start_ms | t_end_ms | score | logits_json | threshold | saliency_uri | created_at
Why: unify classic + ML outputs, make fusion and auditing easy. Your “confidence scoring and anomaly flagging” ends up here. 

audit_events — chain-of-custody
event_id | asset_id | who | when | action | details_json
Why: prove every transformation and access.

reports — exported artifacts
report_id | asset_id | uri_pdf | uri_json | created_at | pipeline_versions_json
Why: evidence packages you can attach or re-generate. Your “Visualization and Reporting” requirement maps here. 

Quick acceptance criteria for each step
Intake: raw file stored under SHA-256 path; audit log written.

Probe/EXIF: raw JSON present; required keys validated; anomalies surfaced.

Shots/frames: every video has ≥1 shot; per-shot frames exist; no decode errors.

Features: ELA/noise/LBP/copy-move artifacts saved with URIs; reproducible from raw. 

Detectors: spatial + temporal scores exist; fusion and calibrated bands defined. 

Reports: heatmaps + evidence in PDF/JSON; includes hashes/tool versions. 

if you want, I can turn this into a Trello-ready backlog (user stories + acceptance criteria) so your team can just pick up cards and build in order.